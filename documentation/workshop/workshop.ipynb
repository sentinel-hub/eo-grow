{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mobile-manor",
   "metadata": {},
   "source": [
    "# `eo-grow` Workshop\n",
    "\n",
    "`eo-grow` is a framework for large-scale processing of EO data. In this workshop we'll learn:\n",
    "\n",
    "- how to run an `eo-grow` pipeline,\n",
    "- how to scale up a pipeline,\n",
    "- how to write a new pipeline.\n",
    "\n",
    "The framework can run:\n",
    "\n",
    "- completely locally on a laptop,\n",
    "- local processing with data storage on S3\n",
    "  * use only for small data transfers!\n",
    "- processing on EC2 instances with data storage on S3.\n",
    "\n",
    "For this workshop we'll use 2nd and 3rd option.\n",
    "\n",
    "\n",
    "## 0. Prerequisites\n",
    "\n",
    "The package requires Python version `>=3.8`. For now it also requires installation from develop branches:\n",
    "\n",
    "- install `eo-learn` from [`develop-v1.0` branch](https://github.com/sentinel-hub/eo-learn/tree/develop-v1.0):\n",
    "   \n",
    "  ```\n",
    "  git clone -b develop-v1.0 --depth 1 https://github.com/sentinel-hub/eo-learn.git\n",
    "  python install_all.py -e\n",
    "  ```\n",
    "\n",
    "- install `eo-grow` from the current branch with:\n",
    "\n",
    "  ```\n",
    "  pip install -e .\n",
    "  ```\n",
    "  \n",
    "This workshop also requires an access to an AWS S3 bucket with data:\n",
    "\n",
    "```\n",
    "aws configure --profile workshop\n",
    "```\n",
    "\n",
    "Additionally you have to set `sentinelhub-py` OAuth credentials.\n",
    "\n",
    "  \n",
    "## 1. How to use `eo-grow`?\n",
    "\n",
    "The core `eo-grow` structure looks like this:\n",
    "\n",
    "![](../eo-grow.png)\n",
    "\n",
    "- A `Pipeline` obtains configuration parameters and uses managers as helpers.\n",
    "- Configuration parameters can be read from JSON files or Python dictionaries. They are parsed with a special [config language](../config-language.md) and wrapped with a `Config` class.\n",
    "- Storage structure and credentials are handled by a `StorageManager`.\n",
    "- AOI is buffered and split into a tiling grid with different implementations of `AreaManager`.\n",
    "- EOPatch naming conventions are defined in an `EOPatchManager`.\n",
    "- Logging is controlled with a `LoggingManager`.\n",
    "\n",
    "Pipeline and manager classes all inherit from a base `EOGrowObject` and are similar in a ways that:\n",
    "\n",
    "- they all contain their own `Schema` class that defines which config parameters they use,\n",
    "- they are all meant to be inherited and customized for any use case.\n",
    "\n",
    "\n",
    "The most basic procedure of using `eo-grow` is:\n",
    "\n",
    "1. set up a project folder for storage,\n",
    "2. implement a new pipeline or use one of the basic pipelines in `eogrow.pipelines`,\n",
    "3. prepare a config file,\n",
    "4. run a pipeline.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "- As a storage we will use a project folder in an AWS S3 bucket `s3://eogrow-workshop/project/`.\n",
    "\n",
    "- We will run a basic download pipeline (`eogrow.pipelines.download.DownloadPipeline`) for AOI defined in a file `s3://eogrow-workshop/project/input-data/bohinj_aoi.geojson`.\n",
    "\n",
    "- We will buffer AOI by `0.01` and split AOI into a UTM grid with a patch size `250x250` pixels on `10m` resolution.\n",
    "\n",
    "\n",
    "For now we will only use CLI commands to run the pipeline. `eo-grow` offers the following commands:\n",
    "\n",
    "- `eogrow` - run a pipeline\n",
    "- `eogrow-template` - create a template config for a\n",
    "- `eogrow-validate` - validate a pipeline config\n",
    "- `eogrow-test` - test managers on a dummy pipeline\n",
    "- `eogrow-ray` - run a pipeline on a cluster\n",
    "\n",
    "Note: names of these commands are defined in `setup.py`.\n",
    "\n",
    "A command `eogrow-template` can help us write a config file. Let's check what templates we get for different objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "!eogrow-template eogrow.pipelines.download.DownloadPipeline\n",
    "# !eogrow-template eogrow.pipelines.download.DownloadPipeline download_template_openapi.json -f\n",
    "\n",
    "# !eogrow-template eogrow.core.storage.StorageManager\n",
    "# !eogrow-template eogrow.core.area.UtmZoneAreaManager\n",
    "# !eogrow-template eogrow.core.eopatch.EOPatchManager\n",
    "# !eogrow-template eogrow.core.logging.LoggingManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-ethnic",
   "metadata": {},
   "source": [
    "We can use config language to:\n",
    "\n",
    "- split config parameters into multiple files,\n",
    "- avoid parameter duplications,\n",
    "- reference:\n",
    "  * relative file paths,\n",
    "  * package import paths,\n",
    "  * environmental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-collector",
   "metadata": {},
   "source": [
    "If we would like to just check if the config file contains correct parameters without running a pipeline we can do that with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "!eogrow-validate configs/download.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-clinton",
   "metadata": {},
   "source": [
    "Before we run the pipeline let's check if all managers are working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "!eogrow-test configs/download.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-freedom",
   "metadata": {},
   "source": [
    "This ran a simple `TestPipeline` that only checked all managers. The pipeline produced\n",
    "\n",
    "- logs\n",
    "- cached area manager buffered shape and grid\n",
    "\n",
    "Let's download cached data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://eogrow-workshop/project/cache/ ./cache --profile workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-finger",
   "metadata": {},
   "source": [
    "To test if the download pipeline will produce correct results we can first run it for a single patch in the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "!eogrow configs/download.json -t 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-investigation",
   "metadata": {},
   "source": [
    "Now we are ready to run it for the entire grid with a command:\n",
    "\n",
    "```\n",
    "eogrow download.json\n",
    "```\n",
    "\n",
    "But before we do this, let's switch to a Ray cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-bearing",
   "metadata": {},
   "source": [
    "## 2. How to scale up?\n",
    "\n",
    "In `eo-grow` parallelization can be achieved with:\n",
    "\n",
    "- multiprocessing on a single machine (for simple use cases),\n",
    "- Ray parallelization on:\n",
    "  * a single machine\n",
    "  * a **cluster of AWS EC2 instances**.\n",
    "\n",
    "Ray cluster can be fully configured with a single YAML file as described in [Ray documentation](https://docs.ray.io/en/latest/cluster/config.html).\n",
    "\n",
    "Once we prepared the YAML file we can spawn a ray cluster:\n",
    "\n",
    "```bash\n",
    "ray up cluster.yaml -y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-invitation",
   "metadata": {},
   "source": [
    "We can attach to it with:\n",
    "\n",
    "```bash\n",
    "ray attach cluster.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-prairie",
   "metadata": {},
   "source": [
    "We can upload any local files to the cluster.\n",
    "\n",
    "```bash\n",
    "ray rsync_up cluster.yaml '/local/path' '/full/absolute/path/on/cluster'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-amount",
   "metadata": {},
   "source": [
    "Note: Alternativelly, we could commit local files and let the cluster pull them from a git repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-gibson",
   "metadata": {},
   "source": [
    "On a cluster we can then simply run the pipeline with:\n",
    "    \n",
    "```bash\n",
    "eogrow download.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-intake",
   "metadata": {},
   "source": [
    "An even easier option is simply run a pipeline on a cluster using your local config to a cluster with a command:\n",
    "\n",
    "```bash\n",
    "eogrow-ray cluster.yaml configs/download.json\n",
    "```\n",
    "\n",
    "This command also has a few useful optional flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "!eogrow-ray --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-laundry",
   "metadata": {},
   "source": [
    "Cluster CPU and memory usage can be monitored from a Ray dashboard. We can connect to it with:\n",
    "\n",
    "```bash\n",
    "ray dashboard cluster.yaml\n",
    "```\n",
    "\n",
    "The dashboard will become available at `localhost:8265`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-florence",
   "metadata": {},
   "source": [
    "When we are done processing, let's make sure that we shut down the cluster:\n",
    "\n",
    "```bash\n",
    "ray down cluster.yaml -y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-programming",
   "metadata": {},
   "source": [
    "## 3. How to implement a new pipeline?\n",
    "\n",
    "Let's start from a typical workflow, which can be created in a prototype phase. The following workflow performs a simple water detection algorithm on a stack of data that we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-emerald",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from eolearn.core import (\n",
    "    EOPatch,\n",
    "    EONode,\n",
    "    FeatureType,\n",
    "    LoadTask,\n",
    "    MapFeatureTask,\n",
    "    OutputTask,\n",
    "    EOWorkflow,\n",
    "    linearly_connect_tasks,\n",
    "    OverwritePermission,\n",
    "    SaveTask,\n",
    ")\n",
    "from eolearn.core.utils.fs import get_aws_credentials\n",
    "from eolearn.features import NormalizedDifferenceIndexTask\n",
    "\n",
    "\n",
    "config = get_aws_credentials(aws_profile=\"workshop\")\n",
    "\n",
    "bands_feature = FeatureType.DATA, \"BANDS\"\n",
    "ndwi_feature = FeatureType.DATA, \"NDWI\"\n",
    "water_feature = FeatureType.MASK_TIMELESS, \"WATER\"\n",
    "\n",
    "load_task = LoadTask(\"s3://eogrow-workshop/project/data/2021-06/\", config=config)\n",
    "\n",
    "ndwi_task = NormalizedDifferenceIndexTask(bands_feature, ndwi_feature, bands=[2, 7])\n",
    "\n",
    "\n",
    "class ThresholdWater(MapFeatureTask):\n",
    "    def map_method(self, ndwi, threshold):\n",
    "        max_ndwi = np.max(ndwi, axis=0)\n",
    "        water = max_ndwi > threshold\n",
    "        return water\n",
    "\n",
    "\n",
    "threshold_task = ThresholdWater(ndwi_feature, water_feature, threshold=0.1)\n",
    "\n",
    "output_task = OutputTask(name=\"result_eop\")\n",
    "\n",
    "nodes = linearly_connect_tasks(load_task, ndwi_task, threshold_task, output_task)\n",
    "workflow = EOWorkflow(nodes)\n",
    "\n",
    "workflow_results = workflow.execute({nodes[0]: {\"eopatch_folder\": \"eopatch-id-08-col-3-row-1\"}})\n",
    "\n",
    "eop = workflow_results.outputs[\"result_eop\"]\n",
    "\n",
    "eop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndwi = eop[ndwi_feature]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "for index in range(12):\n",
    "    ax = axes[index // 4][index % 4]\n",
    "    ax.imshow(ndwi[index, ...], vmin=0.1, vmax=0.5)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "\n",
    "water = eop[water_feature]\n",
    "\n",
    "ax.imshow(water)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-minutes",
   "metadata": {},
   "source": [
    "Now let's put this process into a pipeline. The minimum that we have to do is:\n",
    "\n",
    "- Create a class that inherits from `Pipeline` class.\n",
    "- In case you want to have custom config parameters, add `Schema` subclass that inherits from `Pipeline.Schema`.\n",
    "- Implement `build_workflow` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eogrow.core.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class WaterDetectionPipeline(Pipeline):\n",
    "    class Schema(Pipeline.Schema):\n",
    "        threshold: float\n",
    "\n",
    "    def build_workflow(self):\n",
    "\n",
    "        bands_feature = FeatureType.DATA, \"BANDS\"\n",
    "        ndwi_feature = FeatureType.DATA, \"NDWI\"\n",
    "        water_feature = FeatureType.MASK_TIMELESS, \"WATER\"\n",
    "\n",
    "        load_task = LoadTask(self.storage.get_folder(\"data\", full_path=True), config=self.sh_config)\n",
    "\n",
    "        ndwi_task = NormalizedDifferenceIndexTask(bands_feature, ndwi_feature, bands=[2, 7])\n",
    "\n",
    "        threshold_task = ThresholdWater(ndwi_feature, water_feature, threshold=self.config.threshold)\n",
    "\n",
    "        save_task = SaveTask(\n",
    "            self.storage.get_folder(\"results\", full_path=True),\n",
    "            features=[water_feature, FeatureType.BBOX],\n",
    "            compress_level=1,\n",
    "            overwrite_permission=OverwritePermission.OVERWRITE_FEATURES,\n",
    "            config=self.sh_config,\n",
    "        )\n",
    "\n",
    "        nodes = linearly_connect_tasks(load_task, ndwi_task, threshold_task, output_task)\n",
    "        return EOWorkflow(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-command",
   "metadata": {},
   "source": [
    "This time we cannot run `WaterPipeline` with CLI because the pipeline is implemented in a notebook and we cannot reference its import path. But we can run it from Python. Let's create a config for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eogrow.core.config import Config\n",
    "\n",
    "\n",
    "config = Config.from_path(\"./configs/water_detection.json\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-client",
   "metadata": {},
   "source": [
    "Let's initialize the pipeline and check some of its basic functionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = WaterDetectionPipeline(config)\n",
    "\n",
    "pipeline\n",
    "\n",
    "# pipeline.config\n",
    "# pipeline.sh_config\n",
    "\n",
    "# pipeline.storage\n",
    "# pipeline.storage.filesystem\n",
    "# pipeline.storage.get_folder('data')\n",
    "\n",
    "# pipeline.area_manager\n",
    "# pipeline.area_manager.get_grid()[0]\n",
    "\n",
    "# pipeline.eopatch_manager\n",
    "# pipeline.eopatch_manager.get_eopatch_filenames()\n",
    "# pipeline.patch_list\n",
    "\n",
    "# pipeline.logging_manager\n",
    "# pipeline.logging_manager.get_pipeline_logs_folder('pipeline-name')\n",
    "# pipeline.get_pipeline_execution_name('2021-10-19')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-richardson",
   "metadata": {},
   "source": [
    "During `Pipeline` class initialization only config is validated and parsed according to schema and managers are initialized. No computation is done yet. Let's run the pipeline for a single `EOPatch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_path(\"./configs/water_detection.json\")\n",
    "\n",
    "config.patch_list = [8]  # References EOPatch 'eopatch-id-08-col-3-row-1'\n",
    "\n",
    "pipeline = WaterDetectionPipeline(config)\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-stick",
   "metadata": {},
   "source": [
    "Before we run the pipeline for all EOPatches let's write another pipeline. This one will not be limited by `EOWorkflow` execution. After all, a pipeline can implement any process!\n",
    "\n",
    "In this example we will create a pipeline that vectorizes water masks, joins vectors from all EOPatches and saves them into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eolearn.geometry import RasterToVectorTask\n",
    "\n",
    "r2v_task = RasterToVectorTask(water_feature, values=[1], raster_dtype=np.uint8)\n",
    "\n",
    "eop = r2v_task.execute(eop)\n",
    "\n",
    "eop.vector_timeless[\"WATER\"].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-armor",
   "metadata": {},
   "source": [
    "This time we also have to implement `run_procedure` method. This is the main method that is triggered by `Pipeline.run` and its default implementation only runs an EOWorkflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import fs\n",
    "\n",
    "from eogrow.core.pipeline import Pipeline\n",
    "from eogrow.utils.fs import LocalFile\n",
    "from eogrow.utils.vector import concat_gdf\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class WaterExportPipeline(Pipeline):\n",
    "\n",
    "    water_feature = FeatureType.MASK_TIMELESS, \"WATER\"\n",
    "    vector_water_feature = FeatureType.VECTOR_TIMELESS, \"WATER\"\n",
    "\n",
    "    def run_procedure(self):\n",
    "        workflow = self.build_workflow()\n",
    "        exec_args = self.get_execution_arguments(workflow)\n",
    "\n",
    "        successful, failed, execution_results = self.run_execution(workflow, exec_args, return_results=True)\n",
    "\n",
    "        gdf_list = []\n",
    "        for result in execution_results:\n",
    "            eopatch = result.outputs.get(\"water-vectors\")\n",
    "            if not eopatch:\n",
    "                continue\n",
    "\n",
    "            gdf_list.append(eopatch[self.vector_water_feature])\n",
    "\n",
    "        if not gdf_list:\n",
    "            return successful, failed\n",
    "\n",
    "        LOGGER.info(\"Preparing joined vector dataset\")\n",
    "        joined_gdf = concat_gdf(gdf_list)  # This assumes all dataframes are in the same CRS!\n",
    "\n",
    "        path = fs.path.combine(self.storage.get_folder(\"vector_results\"), \"water-vectors.gpkg\")\n",
    "        with LocalFile(path, mode=\"w\", filesystem=self.storage.filesystem) as local_file:\n",
    "            joined_gdf.to_file(local_file.path, driver=\"GPKG\", encoding=\"utf-8\")\n",
    "        LOGGER.info(\"Saved stats to %s\", path)\n",
    "\n",
    "        return successful, failed\n",
    "\n",
    "    def build_workflow(self):\n",
    "\n",
    "        load_task = LoadTask(\n",
    "            self.storage.get_folder(\"results\", full_path=True), lazy_loading=True, config=self.sh_config\n",
    "        )\n",
    "\n",
    "        r2v_task = RasterToVectorTask(self.water_feature, values=[1], raster_dtype=np.uint8)\n",
    "\n",
    "        output_task = OutputTask(name=\"water-vectors\", features=[self.vector_water_feature])\n",
    "\n",
    "        nodes = linearly_connect_tasks(load_task, r2v_task, output_task)\n",
    "        return EOWorkflow(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our implementation the pipeline doesn't need any additional parameters\n",
    "config = Config.from_path(\"./configs/global_config.json\")\n",
    "\n",
    "config.patch_list = [8]  # References EOPatch 'eopatch-id-08-col-3-row-1'\n",
    "\n",
    "pipeline = WaterExportPipeline(config)\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-investigator",
   "metadata": {},
   "source": [
    "Finally, let's run these new pipelines on a cluster. We can do this by uploading files to the Ray head node, starting Jupyter and run the notebook. We also create configs folder on the head done becuase it doesn't exist yet.\n",
    "\n",
    "```bash\n",
    "ray rsync_up cluster.yaml eogrow-workshop.ipynb /home/ray/eogrow-workshop.ipynb\n",
    "\n",
    "ray exec cluster.yaml 'mkdir configs'\n",
    "ray rsync_up cluster.yaml configs/global_config.json /home/ray/configs/global_config.json\n",
    "ray rsync_up cluster.yaml configs/water_detection.json /home/ray/configs/water_detection.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-fisher",
   "metadata": {},
   "source": [
    "Jupyter can be started with the following command:\n",
    "\n",
    "```bash\n",
    "ray exec cluster.yaml --port-forward=8889 'docker exec -it gem_container /bin/bash -c \"jupyter notebook --port=8889\"'\n",
    "```\n",
    "\n",
    "Then go to `localhost:8889` and  run the relevant cells in the notebook copy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
